From a55facc6bf481473785b85c162b9d6adfc9252cb Mon Sep 17 00:00:00 2001
From: Jiayao Lin <jiayaolinn@gmail.com>
Date: Wed, 4 Mar 2020 14:26:52 -0500
Subject: [PATCH] Enable liftoff on s390x

---
 src/codegen/s390/assembler-s390.h             |    1 +
 src/codegen/s390/macro-assembler-s390.cc      |  172 +++
 src/codegen/s390/macro-assembler-s390.h       |    9 +
 src/flags/flag-definitions.h                  |    2 +-
 src/wasm/baseline/liftoff-assembler-defs.h    |   20 +
 src/wasm/baseline/liftoff-compiler.cc         |   29 +
 .../baseline/s390/liftoff-assembler-s390.h    | 1248 ++++++++++++++---
 src/wasm/wasm-value.h                         |    4 +-
 8 files changed, 1320 insertions(+), 165 deletions(-)

diff --git a/src/codegen/s390/assembler-s390.h b/src/codegen/s390/assembler-s390.h
index 933996050c..8a34c554a1 100644
--- a/src/codegen/s390/assembler-s390.h
+++ b/src/codegen/s390/assembler-s390.h
@@ -678,6 +678,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
     FIDBRA_CURRENT_ROUNDING_MODE = 0,
     FIDBRA_ROUND_TO_NEAREST_AWAY_FROM_0 = 1,
     // ...
+    FIDBRA_ROUND_TO_NEAREST_TO_EVEN = 4,
     FIDBRA_ROUND_TOWARD_0 = 5,
     FIDBRA_ROUND_TOWARD_POS_INF = 6,
     FIDBRA_ROUND_TOWARD_NEG_INF = 7
diff --git a/src/codegen/s390/macro-assembler-s390.cc b/src/codegen/s390/macro-assembler-s390.cc
index c540a5773b..5c311b5591 100644
--- a/src/codegen/s390/macro-assembler-s390.cc
+++ b/src/codegen/s390/macro-assembler-s390.cc
@@ -1601,6 +1601,10 @@ void TurboAssembler::Assert(Condition cond, AbortReason reason, CRegister cr) {
   if (emit_debug_code()) Check(cond, reason, cr);
 }
 
+void TurboAssembler::AssertUnreachable(AbortReason reason) {
+  if (emit_debug_code()) Abort(reason);
+}
+
 void TurboAssembler::Check(Condition cond, AbortReason reason, CRegister cr) {
   Label L;
   b(cond, &L);
@@ -1756,6 +1760,174 @@ void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
   }
 }
 
+void TurboAssembler::FloatMax(DoubleRegister result_reg, DoubleRegister left_reg,
+                              DoubleRegister right_reg) {
+    Label check_nan_left, check_zero, return_left, return_right, done;
+    cebr(left_reg, right_reg);
+    bunordered(&check_nan_left, Label::kNear);
+    beq(&check_zero);
+    bge(&return_left, Label::kNear);
+    b(&return_right, Label::kNear);
+
+    bind(&check_zero);
+    lzdr(kDoubleRegZero);
+    cebr(left_reg, kDoubleRegZero);
+    /* left == right != 0. */
+    bne(&return_left, Label::kNear);
+    /* At this point, both left and right are either 0 or -0. */
+    /* N.B. The following works because +0 + -0 == +0 */
+    /* For max we want logical-and of sign bit: (L + R) */
+    ldr(result_reg, left_reg);
+    aebr(result_reg, right_reg);
+    b(&done, Label::kNear);
+
+    bind(&check_nan_left);
+    cebr(left_reg, left_reg);
+    // left == NaN.                                                  
+    bunordered(&return_left, Label::kNear);
+
+    bind(&return_right);
+    if (right_reg != result_reg) {
+      ldr(result_reg, right_reg);
+    }
+    b(&done, Label::kNear);
+
+    bind(&return_left);
+    if (left_reg != result_reg) {
+      ldr(result_reg, left_reg);
+    }
+    bind(&done);
+}
+
+void TurboAssembler::FloatMin(DoubleRegister result_reg, DoubleRegister left_reg,
+                              DoubleRegister right_reg) {
+    Label check_nan_left, check_zero, return_left, return_right, done;
+    cebr(left_reg, right_reg);
+    bunordered(&check_nan_left, Label::kNear);
+    beq(&check_zero);
+    ble(&return_left, Label::kNear);
+    b(&return_right, Label::kNear);
+
+    bind(&check_zero);
+    lzdr(kDoubleRegZero);
+    cebr(left_reg, kDoubleRegZero);
+    // left == right != 0.                                           
+    bne(&return_left, Label::kNear);
+    // At this point, both left and right are either 0 or -0. */       
+    // N.B. The following works because +0 + -0 == +0 */               
+    // For min we want logical-or of sign bit: -(-L + -R) */           
+    lcebr(left_reg, left_reg);
+    ldr(result_reg, left_reg);
+    if (left_reg == right_reg) {
+      aebr(result_reg, right_reg);
+    } else {
+      sebr(result_reg, right_reg);
+    }
+    lcebr(result_reg, result_reg);
+    b(&done, Label::kNear);
+
+    bind(&check_nan_left);
+    cebr(left_reg, left_reg);
+    /* left == NaN. */
+    bunordered(&return_left, Label::kNear);
+
+    bind(&return_right);
+    if (right_reg != result_reg) {
+      ldr(result_reg, right_reg);
+    }
+    b(&done, Label::kNear);
+
+    bind(&return_left);
+    if (left_reg != result_reg) {
+      ldr(result_reg, left_reg);
+    }
+    bind(&done);
+}
+
+void TurboAssembler::DoubleMax(DoubleRegister result_reg, DoubleRegister left_reg,
+                              DoubleRegister right_reg) {
+    Label check_nan_left, check_zero, return_left, return_right, done;
+    cdbr(left_reg, right_reg);
+    bunordered(&check_nan_left, Label::kNear);
+    beq(&check_zero);
+    bge(&return_left, Label::kNear);
+    b(&return_right, Label::kNear);
+
+    bind(&check_zero);
+    lzdr(kDoubleRegZero);
+    cdbr(left_reg, kDoubleRegZero);
+    /* left == right != 0. */
+    bne(&return_left, Label::kNear);
+    /* At this point, both left and right are either 0 or -0. */
+    /* N.B. The following works because +0 + -0 == +0 */
+    /* For max we want logical-and of sign bit: (L + R) */
+    ldr(result_reg, left_reg);
+    adbr(result_reg, right_reg);
+    b(&done, Label::kNear);
+
+    bind(&check_nan_left);
+    cdbr(left_reg, left_reg);
+    /* left == NaN. */
+    bunordered(&return_left, Label::kNear);
+
+    bind(&return_right);
+    if (right_reg != result_reg) {
+      ldr(result_reg, right_reg);
+    }
+    b(&done, Label::kNear);
+
+    bind(&return_left);
+    if (left_reg != result_reg) {
+      ldr(result_reg, left_reg);
+    }
+    bind(&done);
+}
+
+void TurboAssembler::DoubleMin(DoubleRegister result_reg, DoubleRegister left_reg,
+                              DoubleRegister right_reg) {
+    Label check_nan_left, check_zero, return_left, return_right, done;
+    cdbr(left_reg, right_reg);
+    bunordered(&check_nan_left, Label::kNear);
+    beq(&check_zero);
+    ble(&return_left, Label::kNear);
+    b(&return_right, Label::kNear);
+
+    bind(&check_zero);
+    lzdr(kDoubleRegZero);
+    cdbr(left_reg, kDoubleRegZero);
+    /* left == right != 0. */
+    bne(&return_left, Label::kNear);
+    /* At this point, both left and right are either 0 or -0. */
+    /* N.B. The following works because +0 + -0 == +0 */
+    /* For min we want logical-or of sign bit: -(-L + -R) */
+    lcdbr(left_reg, left_reg);
+    ldr(result_reg, left_reg);
+    if (left_reg == right_reg) {
+      adbr(result_reg, right_reg);
+    } else {
+      sdbr(result_reg, right_reg);
+    }
+    lcdbr(result_reg, result_reg);
+    b(&done, Label::kNear);
+
+    bind(&check_nan_left);
+    cdbr(left_reg, left_reg);
+    /* left == NaN. */
+    bunordered(&return_left, Label::kNear);
+
+    bind(&return_right);
+    if (right_reg != result_reg) {
+      ldr(result_reg, right_reg);
+    }
+    b(&done, Label::kNear);
+
+    bind(&return_left);
+    if (left_reg != result_reg) {
+      ldr(result_reg, left_reg);
+    }
+    bind(&done);
+}
+
 static const int kRegisterPassedArguments = 5;
 
 int TurboAssembler::CalculateStackPassedWords(int num_reg_arguments,
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index d0d6ca6c06..362272b03b 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -145,6 +145,11 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   }
   void JumpIfEqual(Register x, int32_t y, Label* dest);
   void JumpIfLessThan(Register x, int32_t y, Label* dest);
+  
+  void FloatMax(DoubleRegister result_reg, DoubleRegister left_reg, DoubleRegister right_reg);
+  void FloatMin(DoubleRegister result_reg, DoubleRegister left_reg, DoubleRegister right_reg);
+  void DoubleMax(DoubleRegister result_reg, DoubleRegister left_reg, DoubleRegister right_reg);
+  void DoubleMin(DoubleRegister result_reg, DoubleRegister left_reg, DoubleRegister right_reg);
 
   void Call(Register target);
   void Call(Address target, RelocInfo::Mode rmode, Condition cond = al);
@@ -864,6 +869,10 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // Use --debug_code to enable.
   void Assert(Condition cond, AbortReason reason, CRegister cr = cr7);
 
+  // Like Assert(), but without condition.
+  // Use --debug-code to enable.
+  void AssertUnreachable(AbortReason reason);
+
   // Like Assert(), but always enabled.
   void Check(Condition cond, AbortReason reason, CRegister cr = cr7);
 
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index 24f2732339..dc95df5088 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -691,7 +691,7 @@ DEFINE_INT(trace_wasm_ast_start, 0,
 DEFINE_INT(trace_wasm_ast_end, 0, "end function for wasm AST trace (exclusive)")
 // Enable Liftoff by default on ia32 and x64. More architectures will follow
 // once they are implemented and sufficiently tested.
-#if V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_X64
+#if V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_S390X || V8_TARGET_ARCH_ARM
 DEFINE_BOOL(liftoff, true,
             "enable Liftoff, the baseline compiler for WebAssembly")
 #else
diff --git a/src/wasm/baseline/liftoff-assembler-defs.h b/src/wasm/baseline/liftoff-assembler-defs.h
index 65ea26a6d9..d57bc8d5ab 100644
--- a/src/wasm/baseline/liftoff-assembler-defs.h
+++ b/src/wasm/baseline/liftoff-assembler-defs.h
@@ -68,6 +68,13 @@ constexpr RegList kLiftoffAssemblerFpCacheRegs = CPURegister::ListOf(
     d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d16, d17,
     d18, d19, d20, d21, d22, d23, d24, d25, d26, d27, d28, d29);
 
+#elif V8_TARGET_ARCH_S390X
+
+constexpr RegList kLiftoffAssemblerGpCacheRegs =
+    Register::ListOf(r2, r3, r4, r5, r6, r7, r8, r9, r10);
+
+constexpr RegList kLiftoffAssemblerFpCacheRegs = DoubleRegister::ListOf(
+                d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12);
 #else
 
 constexpr RegList kLiftoffAssemblerGpCacheRegs = 0xff;
@@ -115,6 +122,19 @@ constexpr Condition kUnsignedLessEqual = ls;
 constexpr Condition kUnsignedGreaterThan = hi;
 constexpr Condition kUnsignedGreaterEqual = hs;
 
+#elif V8_TARGET_ARCH_S390X
+
+constexpr Condition kEqual = eq; 
+constexpr Condition kUnequal = ne; 
+constexpr Condition kSignedLessThan = Condition(6);
+constexpr Condition kSignedLessEqual = Condition(3);
+constexpr Condition kSignedGreaterThan =  Condition(9);
+constexpr Condition kSignedGreaterEqual = Condition(5);
+constexpr Condition kUnsignedLessThan = lt; 
+constexpr Condition kUnsignedLessEqual = le; 
+constexpr Condition kUnsignedGreaterThan = gt; 
+constexpr Condition kUnsignedGreaterEqual = ge;
+
 #else
 
 // On unimplemented platforms, just make this compile.
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index a17705b710..bed4325ac3 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -1583,7 +1583,36 @@ class LiftoffCompiler {
       outstanding_op_ = kNoOutstandingOp;
     } else {
       // Otherwise, it's an i32 compare opcode.
+#if V8_TARGET_ARCH_S390X
+      Condition tmp = GetCompareCondition(outstanding_op_);
+      DCHECK(tmp != al);
+      switch (tmp) {
+       case kSignedLessThan:
+           tmp = kUnsignedLessThan;
+           break;
+       case kSignedGreaterThan:
+           tmp = kUnsignedGreaterThan;
+           break;
+       case kSignedLessEqual:
+           tmp = kUnsignedLessEqual;
+           break;
+       case kSignedGreaterEqual:
+           tmp = kUnsignedGreaterEqual;
+           break;
+       case kEqual:
+       case kUnequal:
+       case kUnsignedLessThan:
+       case kUnsignedGreaterThan:
+       case kUnsignedLessEqual:
+       case kUnsignedGreaterEqual:
+           break;
+       default:
+          DCHECK(false);
+      }
+      Condition cond = NegateCondition(tmp);
+#else
       Condition cond = NegateCondition(GetCompareCondition(outstanding_op_));
+#endif
       Register rhs = value;
       Register lhs = __ PopToRegister(LiftoffRegList::ForRegs(rhs)).gp();
       __ emit_cond_jump(cond, &cont_false, kWasmI32, lhs, rhs);
diff --git a/src/wasm/baseline/s390/liftoff-assembler-s390.h b/src/wasm/baseline/s390/liftoff-assembler-s390.h
index 1362b84c29..fd6ba4e4e2 100644
--- a/src/wasm/baseline/s390/liftoff-assembler-s390.h
+++ b/src/wasm/baseline/s390/liftoff-assembler-s390.h
@@ -44,20 +44,95 @@ inline MemOperand GetHalfStackSlot(int offset, RegPairHalf half) {
   return MemOperand(fp, -offset + half_offset);
 }
 
+inline MemOperand GetStackSlot(uint32_t offset) {
+  return MemOperand(fp, -offset);
+}
+
+inline MemOperand GetHalfStackSlot(int offset, RegPairHalf half) {
+  int32_t half_offset =
+      half == kLowWord ? 0 : LiftoffAssembler::kStackSlotSize / 2;
+  return MemOperand(fp, -offset + half_offset);
+}
+
+inline MemOperand GetInstanceOperand() {
+  return GetStackSlot(kInstanceOffset);
+}
+
+inline MemOperand GetMemOp(LiftoffAssembler* assm,
+                           UseScratchRegisterScope* temps, Register addr,
+                           Register offset, int32_t offset_imm) {
+  if (is_uint31(offset_imm)) {
+    if (offset == no_reg) return MemOperand(addr, offset_imm);
+  }
+  // Offset immediate does not fit in 31 bits.
+  Register scratch = temps->Acquire();
+  assm->mov(scratch, Operand(offset_imm));
+  if (offset != no_reg) {
+    assm->Add32(scratch, offset);
+  }
+  return MemOperand(addr, scratch);
+}
+
+inline Condition LiftoffCondToCond(Condition cond) {
+  switch (cond) {
+    case kSignedLessThan:
+      return kUnsignedLessThan;
+    case kSignedLessEqual:
+      return kUnsignedLessEqual;
+    case kSignedGreaterThan:
+      return kUnsignedGreaterThan;
+    case kSignedGreaterEqual:
+      return kUnsignedGreaterEqual;
+    case kEqual:
+    case kUnequal:
+    case kUnsignedLessThan:
+    case kUnsignedLessEqual:
+    case kUnsignedGreaterThan:
+    case kUnsignedGreaterEqual:
+      return cond;
+    default:
+      UNREACHABLE();
+   }
+  }
 }  // namespace liftoff
 
 int LiftoffAssembler::PrepareStackFrame() {
-  bailout(kUnsupportedArchitecture, "PrepareStackFrame");
-  return 0;
+  int offset = pc_offset();
+  lay(sp, MemOperand(sp, -0));
+  nop();
+  return offset;
 }
 
 void LiftoffAssembler::PatchPrepareStackFrame(int offset, int frame_size) {
-  bailout(kUnsupportedArchitecture, "PatchPrepareStackFrame");
+  const int kXRegSizeInBits = 64;
+  const int kXRegSize = kXRegSizeInBits >> 3;
+  static_assert(kStackSlotSize == kXRegSize,
+                "kStackSlotSize must equal kXRegSize");
+
+  frame_size = RoundUp(frame_size, 16);
+  if (!(is_uint12(frame_size) ||
+         (is_uint12(frame_size >> 12) && ((frame_size & 0xFFF) == 0)))) {
+    // Round the stack to a page to try to fit a add/sub immediate.
+    frame_size = RoundUp(frame_size, 0x1000);
+    if (!((is_uint12(frame_size) ||
+         (is_uint12(frame_size >> 12) && ((frame_size & 0xFFF) == 0))))) {
+      // Stack greater than 4M! Because this is a quite improbable case, we
+      // just fallback to TurboFan.
+      bailout(kOtherReason, "Stack too big");
+      return;
+    }
+  }
+  constexpr int kAvailableSpace = 64;
+  Assembler patching_assembler(
+      AssemblerOptions{},
+      ExternalAssemblerBuffer(buffer_start_ + offset, kAvailableSpace));
+  patching_assembler.lay(sp, MemOperand(sp, -frame_size));
+  DCHECK_EQ(6, patching_assembler.pc_offset());
 }
 
 void LiftoffAssembler::FinishCode() {}
 
-void LiftoffAssembler::AbortCompilation() {}
+void LiftoffAssembler::AbortCompilation() { AbortedCodeGeneration(); }
 
 // static
 constexpr int LiftoffAssembler::StaticStackFrameSize() {
@@ -85,46 +160,251 @@ bool LiftoffAssembler::NeedsAlignment(ValueType type) {
 
 void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value,
                                     RelocInfo::Mode rmode) {
-  bailout(kUnsupportedArchitecture, "LoadConstant");
+  switch (value.type()) {
+      case kWasmI32:  {
+        TurboAssembler::mov(reg.gp(), Operand(value.to_i32(), rmode)); //}                    
+        break;
+      }
+      case kWasmI64: {
+        DCHECK(RelocInfo::IsNone(rmode));
+        TurboAssembler::mov(reg.gp(), Operand(value.to_i64()));
+        break;
+      }
+      case kWasmF32: {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        LoadFloat32Literal(reg.fp(), value.to_f32_boxed().get_scalar(), scratch);
+        break;
+      }
+      case kWasmF64: {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        uint64_t int_val = bit_cast<uint64_t, double>(value.to_f64_boxed().get_scalar());
+        LoadDoubleLiteral(reg.fp(), int_val, scratch);
+        break;
+      }
+      default:
+        UNREACHABLE();
+    }
 }
 
 void LiftoffAssembler::LoadFromInstance(Register dst, uint32_t offset,
                                         int size) {
-  bailout(kUnsupportedArchitecture, "LoadFromInstance");
+  DCHECK_LE(offset, kMaxInt);
+  LoadP(dst, liftoff::GetInstanceOperand());
+  DCHECK(size == 4 || size == 8);
+  if (size == 4) {
+    LoadW(dst, MemOperand(dst, offset));
+  } else {
+    LoadP(dst, MemOperand(dst, offset));
+  }
 }
 
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      uint32_t offset) {
-  bailout(kUnsupportedArchitecture, "LoadTaggedPointerFromInstance");
+  LoadFromInstance(dst, offset, kTaggedSize);
 }
 
 void LiftoffAssembler::SpillInstance(Register instance) {
-  bailout(kUnsupportedArchitecture, "SpillInstance");
+  StoreP(instance, liftoff::GetInstanceOperand());
 }
 
 void LiftoffAssembler::FillInstanceInto(Register dst) {
-  bailout(kUnsupportedArchitecture, "FillInstanceInto");
+  LoadP(dst, liftoff::GetInstanceOperand());
 }
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
                                          uint32_t offset_imm,
                                          LiftoffRegList pinned) {
-  bailout(kUnsupportedArchitecture, "LoadTaggedPointer");
+  STATIC_ASSERT(kTaggedSize == kInt64Size);
+  Load(LiftoffRegister(dst), src_addr, offset_reg, offset_imm,
+       LoadType::kI64Load, pinned);
 }
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uint32_t offset_imm,
                             LoadType type, LiftoffRegList pinned,
                             uint32_t* protected_load_pc, bool is_load_mem) {
-  bailout(kUnsupportedArchitecture, "Load");
+  UseScratchRegisterScope temps(this);
+  MemOperand src_op =
+        liftoff::GetMemOp(this, &temps, src_addr, offset_reg, offset_imm);
+  
+  if (protected_load_pc) *protected_load_pc = pc_offset();
+  if(!is_load_mem) {
+    switch (type.value()) {
+      case LoadType::kI32Load8U:
+      case LoadType::kI64Load8U:
+        llgc(dst.gp(), src_op);
+        break;
+      case LoadType::kI32Load8S:
+      case LoadType::kI64Load8S:
+        lgb(dst.gp(), src_op);
+        break;
+      case LoadType::kI32Load16U:
+      case LoadType::kI64Load16U:
+        llgh(dst.gp(), src_op);
+        break;
+      case LoadType::kI32Load16S:
+      case LoadType::kI64Load16S:
+        lgh(dst.gp(), src_op);
+        break;
+      case LoadType::kI64Load32U:
+        llgf(dst.gp(), src_op);
+        break;
+      case LoadType::kI32Load: {
+        LoadW(dst.gp(), src_op);
+        break; }
+      case LoadType::kI64Load32S:
+        LoadW(dst.gp(), src_op);
+        break;
+      case LoadType::kI64Load:
+        LoadP(dst.gp(), src_op);
+        break;
+      case LoadType::kF32Load:
+        LoadFloat32(dst.fp(), src_op);
+        break;
+      case LoadType::kF64Load:
+        LoadDouble(dst.fp(), src_op);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+  if (is_load_mem) {
+   switch (type.value()) {
+      case LoadType::kI32Load8U: {
+        llc(dst.gp(), src_op);
+        break; }
+      case LoadType::kI64Load8U: {
+        llgc(dst.gp(), src_op); //LOAD LOGICAL CHARACTER (64 < 8)
+        break; }
+      case LoadType::kI32Load8S: {
+        lb(dst.gp(), src_op); //LOAD BYTE (32 <- 8)
+        break;}
+      case LoadType::kI64Load8S:{
+        lgb(dst.gp(), src_op);//LOAD BYTE (64 < 8)
+        break; }
+      case LoadType::kI32Load16U:{
+        lrvh(dst.gp(), src_op);
+        llhr(dst.gp(), dst.gp());
+        break;}
+      case LoadType::kI64Load16U:{
+        LoadLogicalReversedHalfWordP(dst.gp(), src_op);
+        break; }
+     case LoadType::kI32Load16S:{
+        lrvh(dst.gp(), src_op);
+        lhr(dst.gp(), dst.gp());
+        break;}
+      case LoadType::kI32Load:{
+        lrv(dst.gp(), src_op);
+        break;}
+      case LoadType::kI64Load16S:{
+        lrvh(dst.gp(), src_op);
+        lghr(dst.gp(), dst.gp());
+        break;}
+      case LoadType::kI64Load32U:{
+        LoadLogicalReversedWordP(dst.gp(), src_op);
+        break;}
+      case LoadType::kI64Load32S:{
+        lrv(dst.gp(), src_op);
+        lgfr(dst.gp(), dst.gp());
+        break;}
+      case LoadType::kI64Load:{
+        lrvg(dst.gp(), src_op);
+        break;}
+      case LoadType::kF64Load:{
+        lrvg(r0, src_op);
+        ldgr(dst.fp(), r0);
+        break;}
+      default:
+        UNREACHABLE();
+    }
+  }
 }
 
 void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
                              uint32_t offset_imm, LiftoffRegister src,
                              StoreType type, LiftoffRegList pinned,
                              uint32_t* protected_store_pc, bool is_store_mem) {
-  bailout(kUnsupportedArchitecture, "Store");
+  UseScratchRegisterScope temps(this);
+  MemOperand dst_op =
+        liftoff::GetMemOp(this, &temps, dst_addr, offset_reg, offset_imm);
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+
+  if (is_store_mem) {
+    switch (type.value()) {
+      case StoreType::kI64Store8:
+      case StoreType::kI32Store8:
+        // No need to change endianness for byte size.
+        break;
+      case StoreType::kF32Store: {
+        MovFloatToInt(r0, src.fp());
+        lrvr(r0, r0); //reverse byte
+        MovIntToFloat(kScratchDoubleReg, r0); //restore to a double reg
+        StoreFloat32(kScratchDoubleReg, dst_op);
+        break; }
+      case StoreType::kI32Store: {
+        strv(src.gp(), dst_op);
+        break;}
+      case StoreType::kI32Store16:{
+        strvh(src.gp(), dst_op);
+        break;}
+      case StoreType::kF64Store:{
+        lgdr(r0, src.fp());
+        lrvgr(r0, r0);
+        ldgr(kScratchDoubleReg, r0);
+        StoreDouble(kScratchDoubleReg, dst_op);
+        break;}
+      case StoreType::kI64Store:{
+        strvg(src.gp(), dst_op);
+        break;}
+      case StoreType::kI64Store32:{
+        strv(src.gp(), dst_op);
+        break;}
+      case StoreType::kI64Store16:{
+        strvh(src.gp(), dst_op);
+        break;}
+      default:
+        UNREACHABLE();
+    }
+  }
+
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+  switch (type.value()) {
+    case StoreType::kI32Store8:
+    case StoreType::kI64Store8:
+      stc(src.gp(), dst_op);
+      break;
+    case StoreType::kI32Store16:
+    case StoreType::kI64Store16: {
+      if (!is_store_mem) {
+          StoreHalfWord(src.gp(), dst_op);
+      }
+      break; }
+    case StoreType::kI32Store:
+    case StoreType::kI64Store32: {
+      if (!is_store_mem) {
+          StoreW(src.gp(), dst_op);}
+      break;}
+    case StoreType::kI64Store:  {
+      if (!is_store_mem) {
+        StoreP(src.gp(), dst_op);
+      }
+      break; }
+    case StoreType::kF32Store: {
+      if (!is_store_mem) {
+          StoreFloat32(src.fp(), dst_op);
+      }
+      break; }
+    case StoreType::kF64Store: {
+      if (!is_store_mem) {
+        StoreDouble(src.fp(), dst_op);
+      }
+      break; }
+    default:
+      UNREACHABLE();
+  }
 }
 
 void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
@@ -142,37 +422,124 @@ void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
 void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,
                                            uint32_t caller_slot_idx,
                                            ValueType type) {
-  bailout(kUnsupportedArchitecture, "LoadCallerFrameSlot");
+  int32_t offset = (caller_slot_idx + 1) * 8;
+  switch (type) {
+    case kWasmI32: {
+      printf("LoadCallerFrameSlot I32\n");
+      l(dst.gp(), MemOperand(fp, offset + 4)); //constexpr int32_t kLowWordOffset = 4;
+      break;}
+    case kWasmI64: {
+      printf("LoadCallerFrameSlot I64\n");
+      LoadP(dst.gp(), MemOperand(fp, offset)); //constexpr int32_t kHighWordOffset = 0;
+      break;}
+    case kWasmF32:{
+      printf("LoadCallerFrameSlot F32\n");
+      LoadFloat32(dst.fp(),MemOperand(fp, offset));
+      break;}
+    case kWasmF64:{
+      printf("LoadCallerFrameSlot F64\n");
+      offset = (caller_slot_idx + 1) * 8 ;
+      LoadDouble(dst.fp(), MemOperand(fp, offset));
+      break;}
+    default:
+      UNREACHABLE();
+  }
 }
 
 void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
                                       ValueType type) {
-  bailout(kUnsupportedArchitecture, "MoveStackValue");
+  DCHECK_NE(dst_offset, src_offset);
+  LiftoffRegister reg = GetUnusedRegister(reg_class_for(type));
+  Fill(reg, src_offset, type);
+  Spill(dst_offset, reg, type);
 }
 
 void LiftoffAssembler::Move(Register dst, Register src, ValueType type) {
-  bailout(kUnsupportedArchitecture, "Move Register");
+  if (type == kWasmI32) {
+   lr(dst, src);
+  } else {
+    DCHECK_EQ(kWasmI64, type);
+    TurboAssembler::Move(dst, src);
+  }
 }
 
 void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
                             ValueType type) {
-  bailout(kUnsupportedArchitecture, "Move DoubleRegister");
+  DCHECK_NE(dst, src);
+  if (type == kWasmF32) {
+    ler(dst, src); 
+  } else {
+    DCHECK_EQ(kWasmF64, type);
+    TurboAssembler::Move(dst, src);
+  }
 }
 
 void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueType type) {
-  bailout(kUnsupportedArchitecture, "Spill register");
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (type) {
+    case kWasmI32:
+      StoreW(reg.gp(), dst);
+      break;
+    case kWasmI64:
+      StoreP(reg.gp(), dst);
+      break;
+    case kWasmF32:
+      StoreFloat32(reg.fp(), dst);
+      break;
+    case kWasmF64:
+      StoreDouble(reg.fp(), dst);
+      break;
+    default:
+      UNREACHABLE();
+  }
 }
 
 void LiftoffAssembler::Spill(int offset, WasmValue value) {
-  bailout(kUnsupportedArchitecture, "Spill value");
+  RecordUsedSpillOffset(offset);
+  UseScratchRegisterScope temps(this);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (value.type()) {
+    case kWasmI32:  {
+      Register src = temps.Acquire();
+      mov(src, Operand(value.to_i32()));
+      StoreW(src, dst);
+      break;
+      }
+    case kWasmI64: {
+      printf("Spill WasmValue I64\n");
+      Register src = temps.Acquire();
+      mov(src, Operand(value.to_i64()));
+      StoreP(src, dst);
+      break;
+    }
+    default:
+      // We do not track f32 and f64 constants, hence they are unreachable.
+      UNREACHABLE();
+  }
 }
 
 void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueType type) {
-  bailout(kUnsupportedArchitecture, "Fill");
+  switch (type) {
+    case kWasmI32:
+      LoadW(reg.gp(), MemOperand(fp, -offset));
+      break;
+    case kWasmI64:
+      LoadP(reg.gp(), liftoff::GetStackSlot(offset));
+      break;
+    case kWasmF32:
+      LoadFloat32(reg.fp(), liftoff::GetStackSlot(offset));
+      break;
+    case kWasmF64:
+      LoadDouble(reg.fp(), liftoff::GetStackSlot(offset));
+      break;
+    default:
+      UNREACHABLE();
+  }
 }
 
 void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {
-  bailout(kUnsupportedArchitecture, "FillI64Half");
+  UNREACHABLE();
 }
 
 void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
@@ -184,9 +551,7 @@ void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
   push(r0);
   mov(r0, Operand(0));
 
-  if (size <= 5 * kStackSlotSize) {
-    // Special straight-line code for up to five slots. Generates two
-    // instructions per slot.
+  if (size <= 12 * kStackSlotSize) {
     uint32_t remainder = size;
     for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {
       StoreP(r0, liftoff::GetHalfStackSlot(start + remainder, kLowWord));
@@ -194,325 +559,738 @@ void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
     }
     DCHECK(remainder == 4 || remainder == 0);
     if (remainder) {
-      StoreP(r0, liftoff::GetHalfStackSlot(start + remainder, kLowWord));
+      StoreW(r0, liftoff::GetStackSlot(start + remainder)); //TODO
     }
   } else {
-    // General case for bigger counts (9 instructions).
     // Use r3 for start address (inclusive), r4 for end address (exclusive).
-    push(r3);
-    push(r4);
-    SubP(r3, fp, Operand(start + size));
-    SubP(r4, fp, Operand(start));
+    Push(r3, r4);
+    SubP(r3, fp, Operand(start + size)); //AddP(r3, fp, Operand(-start-size));
+    SubP(r4, fp, Operand(start)); //AddP(r4, fp, Operand(-start));
 
     Label loop;
     bind(&loop);
-    StoreP(r0, MemOperand(r0));
-    la(r0, MemOperand(r0, kSystemPointerSize));
+    // StoreP(r0, MemOperand(r0));
+    // la(r0, MemOperand(r0, kSystemPointerSize));
+    StoreP(r0, MemOperand(r3, kSystemPointerSize));
+    lay(r3, MemOperand(r3, kSystemPointerSize));
     CmpLogicalP(r3, r4);
     bne(&loop);
 
-    pop(r4);
-    pop(r3);
+    Pop(r3, r4);
+    // pop(r4);
+    // pop(r3);
   }
-
   pop(r0);
 }
 
-#define UNIMPLEMENTED_I32_BINOP(name)                            \
+#define I32_BINOP(name, instruction)                             \
   void LiftoffAssembler::emit_##name(Register dst, Register lhs, \
                                      Register rhs) {             \
-    bailout(kUnsupportedArchitecture, "i32 binop: " #name);      \
+    instruction(dst, lhs, rhs);                                  \
   }
-#define UNIMPLEMENTED_I32_BINOP_I(name)                          \
-  UNIMPLEMENTED_I32_BINOP(name)                                  \
+#define I32_BINOP_I(name, instruction)                           \
+  I32_BINOP(name, instruction)                                   \
   void LiftoffAssembler::emit_##name(Register dst, Register lhs, \
                                      int32_t imm) {              \
-    bailout(kUnsupportedArchitecture, "i32 binop_i: " #name);    \
+    instruction(dst, lhs, Operand(imm));                         \
   }
-#define UNIMPLEMENTED_I64_BINOP(name)                                          \
+#define I64_BINOP(name, instruction)                                           \
   void LiftoffAssembler::emit_##name(LiftoffRegister dst, LiftoffRegister lhs, \
                                      LiftoffRegister rhs) {                    \
-    bailout(kUnsupportedArchitecture, "i64 binop: " #name);                    \
+    instruction(dst.gp(), lhs.gp(), rhs.gp());                                 \
   }
-#define UNIMPLEMENTED_I64_BINOP_I(name)                                        \
-  UNIMPLEMENTED_I64_BINOP(name)                                                \
+#define I64_BINOP_I(name, instruction)                                         \
+  I64_BINOP(name, instruction)                                                 \
   void LiftoffAssembler::emit_##name(LiftoffRegister dst, LiftoffRegister lhs, \
                                      int32_t imm) {                            \
-    bailout(kUnsupportedArchitecture, "i64 binop_i: " #name);                  \
-  }
-#define UNIMPLEMENTED_GP_UNOP(name)                                \
-  void LiftoffAssembler::emit_##name(Register dst, Register src) { \
-    bailout(kUnsupportedArchitecture, "gp unop: " #name);          \
+    instruction(dst.gp(), lhs.gp(), Operand(imm));                             \
   }
-#define UNIMPLEMENTED_FP_BINOP(name)                                         \
+#define FP32_BINOP(name, instruction)                                        \
   void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister lhs, \
                                      DoubleRegister rhs) {                   \
-    bailout(kUnsupportedArchitecture, "fp binop: " #name);                   \
+    ler(dst, lhs);                                                           \
+    instruction(dst, rhs);                                                   \
   }
-#define UNIMPLEMENTED_FP_UNOP(name)                                            \
+#define FP32_UNOP(name, instruction)                                           \
   void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
-    bailout(kUnsupportedArchitecture, "fp unop: " #name);                      \
+    instruction(dst,src);                                                      \
   }
-#define UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(name)                                \
-  bool LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
-    bailout(kUnsupportedArchitecture, "fp unop: " #name);                      \
-    return true;                                                               \
+#define FP64_BINOP(name, instruction)                                        \
+  void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister lhs, \
+                                     DoubleRegister rhs) {                   \
+    ldr(dst, lhs);                                                           \
+    instruction(dst, rhs);                                                   \
   }
-#define UNIMPLEMENTED_I32_SHIFTOP(name)                          \
+#define FP64_UNOP(name, instruction)                                           \
+  void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    instruction(dst, src);                                                     \
+  }
+#define I32_SHIFTOP(name, instruction)                           \
   void LiftoffAssembler::emit_##name(Register dst, Register src, \
                                      Register amount) {          \
-    bailout(kUnsupportedArchitecture, "i32 shiftop: " #name);    \
+    LoadRR(r0, src);                                             \
+    LoadRR(r1, amount);                                          \
+    And(amount, Operand(0x1f));                                  \
+    instruction(dst, r0, amount);                                \
+    LoadRR(src, r0 );                                            \
+    LoadRR(amount, r1);                                          \
   }                                                              \
   void LiftoffAssembler::emit_##name(Register dst, Register src, \
                                      int32_t amount) {           \
-    bailout(kUnsupportedArchitecture, "i32 shiftop: " #name);    \
+    instruction(dst, src, Operand(amount & 31));                 \
   }
-#define UNIMPLEMENTED_I64_SHIFTOP(name)                                        \
+#define I64_SHIFTOP(name, instruction)                                         \
   void LiftoffAssembler::emit_##name(LiftoffRegister dst, LiftoffRegister src, \
                                      Register amount) {                        \
-    bailout(kUnsupportedArchitecture, "i64 shiftop: " #name);                  \
+    LoadRR(r0, src.gp());                                                      \
+    LoadRR(r1, amount);                                                        \
+    AndP(amount, Operand(0x3f));                                               \
+    instruction(dst.gp(), r0, amount);                                         \
+    LoadRR(src.gp(), r0);                                                      \
+    LoadRR(amount, r1);                                                        \
   }                                                                            \
   void LiftoffAssembler::emit_##name(LiftoffRegister dst, LiftoffRegister src, \
                                      int32_t amount) {                         \
-    bailout(kUnsupportedArchitecture, "i64 shiftop: " #name);                  \
-  }
-
-UNIMPLEMENTED_I32_BINOP_I(i32_add)
-UNIMPLEMENTED_I32_BINOP(i32_sub)
-UNIMPLEMENTED_I32_BINOP(i32_mul)
-UNIMPLEMENTED_I32_BINOP_I(i32_and)
-UNIMPLEMENTED_I32_BINOP_I(i32_or)
-UNIMPLEMENTED_I32_BINOP_I(i32_xor)
-UNIMPLEMENTED_I32_SHIFTOP(i32_shl)
-UNIMPLEMENTED_I32_SHIFTOP(i32_sar)
-UNIMPLEMENTED_I32_SHIFTOP(i32_shr)
-UNIMPLEMENTED_I64_BINOP_I(i64_add)
-UNIMPLEMENTED_I64_BINOP(i64_sub)
-UNIMPLEMENTED_I64_BINOP(i64_mul)
+    instruction(dst.gp(), src.gp(), Operand(amount & 63));                     \
+  }
+
+I32_BINOP_I(i32_add, AddP) //Add32
+I32_BINOP(i32_sub, Sub32)
+I32_BINOP(i32_mul, Mul)
+I32_BINOP_I(i32_and, And)
+I32_BINOP_I(i32_or, Or)
+I32_BINOP_I(i32_xor, Xor)
+I32_SHIFTOP(i32_shl, ShiftLeft)
+I32_SHIFTOP(i32_sar, ShiftRightArith)
+I32_SHIFTOP(i32_shr, ShiftRight)
+I64_BINOP_I(i64_add, AddP)
+I64_BINOP(i64_sub, SubP)
+I64_BINOP(i64_mul, Mul)
 #ifdef V8_TARGET_ARCH_S390X
-UNIMPLEMENTED_I64_BINOP_I(i64_and)
-UNIMPLEMENTED_I64_BINOP_I(i64_or)
-UNIMPLEMENTED_I64_BINOP_I(i64_xor)
+I64_BINOP_I(i64_and, AndP)
+I64_BINOP_I(i64_or, OrP)
+I64_BINOP_I(i64_xor, XorP)
 #endif
-UNIMPLEMENTED_I64_SHIFTOP(i64_shl)
-UNIMPLEMENTED_I64_SHIFTOP(i64_sar)
-UNIMPLEMENTED_I64_SHIFTOP(i64_shr)
-UNIMPLEMENTED_GP_UNOP(i32_clz)
-UNIMPLEMENTED_GP_UNOP(i32_ctz)
-UNIMPLEMENTED_FP_BINOP(f32_add)
-UNIMPLEMENTED_FP_BINOP(f32_sub)
-UNIMPLEMENTED_FP_BINOP(f32_mul)
-UNIMPLEMENTED_FP_BINOP(f32_div)
-UNIMPLEMENTED_FP_BINOP(f32_min)
-UNIMPLEMENTED_FP_BINOP(f32_max)
-UNIMPLEMENTED_FP_BINOP(f32_copysign)
-UNIMPLEMENTED_FP_UNOP(f32_abs)
-UNIMPLEMENTED_FP_UNOP(f32_neg)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f32_ceil)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f32_floor)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f32_trunc)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f32_nearest_int)
-UNIMPLEMENTED_FP_UNOP(f32_sqrt)
-UNIMPLEMENTED_FP_BINOP(f64_add)
-UNIMPLEMENTED_FP_BINOP(f64_sub)
-UNIMPLEMENTED_FP_BINOP(f64_mul)
-UNIMPLEMENTED_FP_BINOP(f64_div)
-UNIMPLEMENTED_FP_BINOP(f64_min)
-UNIMPLEMENTED_FP_BINOP(f64_max)
-UNIMPLEMENTED_FP_BINOP(f64_copysign)
-UNIMPLEMENTED_FP_UNOP(f64_abs)
-UNIMPLEMENTED_FP_UNOP(f64_neg)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f64_ceil)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f64_floor)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f64_trunc)
-UNIMPLEMENTED_FP_UNOP_RETURN_TRUE(f64_nearest_int)
-UNIMPLEMENTED_FP_UNOP(f64_sqrt)
-
-#undef UNIMPLEMENTED_I32_BINOP
-#undef UNIMPLEMENTED_I32_BINOP_I
-#undef UNIMPLEMENTED_I64_BINOP
-#undef UNIMPLEMENTED_I64_BINOP_I
+I64_SHIFTOP(i64_shl, ShiftLeftP) 
+I64_SHIFTOP(i64_sar, ShiftRightArithP)
+I64_SHIFTOP(i64_shr, ShiftRightP) 
+FP32_BINOP(f32_add, aebr)
+FP32_BINOP(f32_sub, sebr)
+FP32_BINOP(f32_mul, meebr)
+FP32_BINOP(f32_div, debr)
+FP32_UNOP(f32_abs, lpebr)
+FP32_UNOP(f32_neg, lcebr)
+FP32_UNOP(f32_sqrt, sqebr) 
+FP64_BINOP(f64_add, adbr)
+FP64_BINOP(f64_sub, sdbr)
+FP64_BINOP(f64_mul, mdbr)
+FP64_BINOP(f64_div, ddbr)
+FP64_UNOP(f64_abs, lpdbr)
+FP64_UNOP(f64_neg, lcdbr)
+FP64_UNOP(f64_sqrt, sqdbr)
+
+#undef I32_BINOP
+#undef I32_BINOP_I
+#undef I64_BINOP
+#undef I64_BINOP_I
 #undef UNIMPLEMENTED_GP_UNOP
-#undef UNIMPLEMENTED_FP_BINOP
-#undef UNIMPLEMENTED_FP_UNOP
+#undef FP32_BINOP
+#undef FP32_UNOP
+#undef FP64_BINOP
+#undef UNIMPLEMENTED_FP64_UNOP
 #undef UNIMPLEMENTED_FP_UNOP_RETURN_TRUE
-#undef UNIMPLEMENTED_I32_SHIFTOP
-#undef UNIMPLEMENTED_I64_SHIFTOP
+#undef I32_SHIFTOP
+#undef I64_SHIFTOP
+
+void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {
+  llgfr(dst, src);
+  flogr(r0, dst);
+  Add32(dst, r0, Operand(-32));
+}
+
+void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {
+  Label cont;
+  Label done;
+  Cmp32(src, Operand(0));
+  bne(&cont);
+  lhi(dst, Operand(32));
+  beq(&done);
+
+  bind(&cont);
+  llgfr(src, src);
+  lcgr(r0, src); //r0 two's complement
+  ngr(src, r0); // src.gp -> 00000100 
+  flogr(r0, src); // xxxx1aa, number of xi
+  lghi(r1, Operand(63));
+  SubP(dst, r1, r0);
+  bind(&done);
+}
 
 bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {
-  bailout(kUnsupportedArchitecture, "i32_popcnt");
+  Popcnt32(dst, src);
   return true;
 }
 
 bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,
                                        LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "i64_popcnt");
+  Popcnt64(dst.gp(), src.gp());
   return true;
 }
 
+bool LiftoffAssembler::emit_f32_ceil(DoubleRegister dst, DoubleRegister src) {
+  fiebra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_POS_INF,
+         dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f32_floor(DoubleRegister dst, DoubleRegister src) {
+  fiebra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_NEG_INF,
+         dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f32_trunc(DoubleRegister dst, DoubleRegister src) {
+  fiebra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_0,
+         dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f32_nearest_int(DoubleRegister dst,
+                                            DoubleRegister src) {
+  fiebra(v8::internal::Assembler::FIDBRA_ROUND_TO_NEAREST_TO_EVEN,
+         dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f64_ceil(DoubleRegister dst, DoubleRegister src) {
+  fidbra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_POS_INF,
+                                  dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f64_floor(DoubleRegister dst, DoubleRegister src) {
+  fidbra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_NEG_INF,
+                                  dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f64_trunc(DoubleRegister dst, DoubleRegister src) {
+  fidbra(v8::internal::Assembler::FIDBRA_ROUND_TOWARD_0,
+                                  dst, src);
+  return true;
+}
+
+bool LiftoffAssembler::emit_f64_nearest_int(DoubleRegister dst,
+                                            DoubleRegister src) {
+  fidbra(v8::internal::Assembler::FIDBRA_ROUND_TO_NEAREST_TO_EVEN,
+                                  dst, src);
+  return true;
+}
+
+void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+    DoubleMin(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+    DoubleMax(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  FloatMin(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  FloatMax(dst, lhs, rhs);
+}
+
+
 void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,
                                      Label* trap_div_by_zero,
                                      Label* trap_div_unrepresentable) {
-  bailout(kUnsupportedArchitecture, "i32_divs");
+  Label cont;
+  Cmp32(rhs, Operand(0));
+  b(eq, trap_div_by_zero);
+  Cmp32(rhs, Operand(-1));
+  bne(&cont);
+  Cmp32(lhs, Operand(kMinInt));
+  b(eq, trap_div_unrepresentable);
+  bind(&cont);
+  Div32(dst, lhs, rhs);
 }
 
 void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i32_divu");
+  CmpLogical32(rhs, Operand(0));
+  beq(trap_div_by_zero);
+  DivU32(dst, lhs, rhs);
 }
 
 void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i32_rems");
+  Label cont;
+  Label done;
+  Label trap_div_unrepresentable;
+  // Check for division by zero.
+  Cmp32(rhs, Operand(0));
+  beq(trap_div_by_zero);
+
+  //Check kMinInt/-1 case.
+  Cmp32(rhs, Operand(-1));
+  bne(&cont);
+  Cmp32(lhs, Operand(kMinInt));
+  beq(&trap_div_unrepresentable);
+
+  bind(&cont); //Continue noraml calculation.
+  Mod32(dst, lhs, rhs);
+  bne(&done);
+
+  bind(&trap_div_unrepresentable);
+  mov(dst, Operand(0));
+  bind(&done);
 }
 
 void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i32_remu");
+  CmpLogical32(rhs, Operand(0));
+  beq(trap_div_by_zero);
+  ModU32(dst, lhs,  rhs);
 }
 
 bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs,
                                      Label* trap_div_by_zero,
                                      Label* trap_div_unrepresentable) {
-  bailout(kUnsupportedArchitecture, "i64_divs");
+  iilf(r0, Operand(0));
+  iihf(r0, Operand(kMinInt));
+
+  Label cont;
+  // Check for division by zero.
+  CmpP(rhs.gp(), Operand(0));
+  beq(trap_div_by_zero);
+
+  // Check for kMinInt / -1. This is unrepresentable.
+  CmpP(rhs.gp(), Operand(-1));
+  bne(&cont);
+  CmpP(lhs.gp(), r0);
+  b(eq, trap_div_unrepresentable);
+
+  bind(&cont);
+  Div64(dst.gp(), lhs.gp(), rhs.gp());
   return true;
 }
 
 bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i64_divu");
+  CmpLogicalP(rhs.gp(), Operand(0)); 
+  b(eq, trap_div_by_zero);
+  DivU64(dst.gp(), lhs.gp(), rhs.gp());
   return true;
 }
 
 bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i64_rems");
+  iilf(r0, Operand(0));
+  iihf(r0, Operand(kMinInt));
+
+  Label cont;
+  Label done;
+  Label trap_div_unrepresentable;
+  // Check for division by zero.
+  CmpP(rhs.gp(), Operand(0));
+  beq(trap_div_by_zero);
+
+  // Check for kMinInt / -1. This is unrepresentable.
+  CmpP(rhs.gp(), Operand(-1));
+  bne(&cont);
+  CmpP(lhs.gp(), r0);
+  b(eq, trap_div_unrepresentable);
+
+  bind(&cont);
+  Mod64(dst.gp(), lhs.gp(), rhs.gp());
+  b(&done);
+
+  bind(&trap_div_unrepresentable);
+  mov(dst.gp(), Operand(0));
+  bind(&done);
   return true;
 }
 
 bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs,
                                      Label* trap_div_by_zero) {
-  bailout(kUnsupportedArchitecture, "i64_remu");
+  // Check for division by zero.
+  CmpLogicalP(rhs.gp(), Operand(0)); 
+  beq(trap_div_by_zero);
+  ModU64(dst.gp(), lhs.gp(), rhs.gp());
   return true;
 }
 
 void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "i64_clz");
+  flogr(r0, src.gp());
+  LoadRR(dst.gp(), r0);
 }
 
 void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "i64_ctz");
+  Label cont;
+  Label done;
+  CmpP(src.gp(), Operand(0));
+  bne(&cont);
+  lghi(dst.gp(), Operand(64));
+  beq(&done);
+
+  bind(&cont);
+  lcgr(r0, src.gp()); //r0 two's complement
+  ngr(src.gp(), r0); // src.gp -> 00000100 
+  flogr(r0, src.gp()); // xxxx1aa, number of xi
+  lghi(r1, Operand(63));
+  SubP(dst.gp(), r1, r0);
+  bind(&done);
 }
 
 void LiftoffAssembler::emit_u32_to_intptr(Register dst, Register src) {
 #ifdef V8_TARGET_ARCH_S390X
-  bailout(kUnsupportedArchitecture, "emit_u32_to_intptr");
+  LoadlW(dst, src);
 #else
 // This is a nop on s390.
 #endif
 }
 
+void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  constexpr uint64_t kF64SignBit = uint64_t{1} << 63;
+  UseScratchRegisterScope temps(this);
+  Register scratch2 = temps.Acquire();
+  MovDoubleToInt64(r0, lhs); //scratch
+  // Clear sign bit in {scratch}.
+  AndP(r0, Operand(~kF64SignBit));
+
+  MovDoubleToInt64(scratch2, rhs);
+  // Isolate sign bit in {scratch2}.
+  AndP(scratch2, Operand(kF64SignBit));
+  // Combine {scratch2} into {scratch}.
+  OrP(r0, r0, scratch2);
+  MovInt64ToDouble(dst, r0);
+}
+
+void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  UseScratchRegisterScope temps(this);
+  Register scratch2 = temps.Acquire();
+  // Extract sign bit from {rhs} into {scratch2}.
+  MovDoubleToInt64(scratch2, rhs);
+  ShiftRightP(scratch2, scratch2, Operand(63));
+  ShiftLeftP(scratch2, scratch2, Operand(63));
+  // Reset sign bit of {lhs} (in {scratch}).
+  MovDoubleToInt64(r0, lhs);
+  ShiftLeftP(r0, r0, Operand(1));
+  ShiftRightP(r0, r0, Operand(1));
+  // Combine both values into {scratch} and move into {dst}. 
+  OrP(r0, r0, scratch2);
+  MovInt64ToDouble(dst, r0);
+}
+
 bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
                                             LiftoffRegister dst,
                                             LiftoffRegister src, Label* trap) {
-  bailout(kUnsupportedArchitecture, "emit_type_conversion");
+  switch (opcode) {
+    case kExprI32ConvertI64:
+      lgfr(dst.gp(), src.gp());
+      return true;
+    case kExprI32SConvertF32: {
+      ConvertFloat32ToInt32(dst.gp(),
+                            src.fp(),
+                            kRoundToZero); // f32 -> i32 round to zero.
+     b(Condition(1), trap);
+      return true;
+    }
+    case kExprI32UConvertF32: {
+      ConvertFloat32ToUnsignedInt32(dst.gp(),
+                                    src.fp()); // f32 -> i32 round to zero.
+      // Check underflow and NaN.
+      b(Condition(1), trap);
+      return true;
+    }
+    case kExprI32SConvertF64: {
+      ConvertDoubleToInt32(dst.gp(),
+                           src.fp()); // f64 -> i32 round to zero.
+      b(Condition(1), trap);
+      return true;
+    }
+    case kExprI32UConvertF64: {
+      ConvertDoubleToUnsignedInt32(dst.gp(), src.fp()); // f64 -> i32 round to zero.
+      b(Condition(1), trap);
+      return true;
+    }
+    case kExprI32ReinterpretF32:
+      //MovFloatToInt(dst.gp(), src.fp());
+      lgdr(dst.gp(), src.fp());
+      srlg(dst.gp(), dst.gp(), Operand(32));
+      return true;
+    case kExprI64SConvertI32:
+      LoadW(dst.gp(), src.gp());
+      return true;
+    case kExprI64UConvertI32:
+      llgfr(dst.gp(), src.gp());
+      //lr(dst.gp(), src.gp());
+      return true;
+    case kExprI64ReinterpretF64:
+      lgdr(dst.gp(), src.fp());
+      return true;
+    case kExprF32SConvertI32: {
+      ConvertIntToFloat(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF32ConvertF64:
+      ledbr(dst.fp(), src.fp());
+      return true;
+    case kExprF32ReinterpretI32: {
+      sllg(r0, src.gp(), Operand(32));
+      ldgr(dst.fp(), r0);
+      return true;
+    }
+    case kExprF64SConvertI32: {
+      ConvertIntToDouble(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF64UConvertI32: {
+    ConvertUnsignedIntToDouble(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF64ConvertF32:
+      ldebr(dst.fp(), src.fp());
+      return true;
+    case kExprF64ReinterpretI64:
+      ldgr(dst.fp(), src.gp());
+      return true;
+    case kExprF64SConvertI64:
+      ConvertInt64ToDouble(dst.fp(), src.gp());
+      return true;
+    case kExprF64UConvertI64:
+      ConvertUnsignedInt64ToDouble(dst.fp(), src.gp());
+      return true;
+    case kExprI64SConvertF32: {
+      ConvertFloat32ToInt64(dst.gp(), src.fp());  // f32 -> i64 round to zero.
+      b(Condition(1), trap);
+      return true;
+    }
+    case kExprI64UConvertF32: {
+      Label done;
+      ConvertFloat32ToUnsignedInt64(dst.gp(), src.fp()); // f32 -> i64 round to zero.
+      b(Condition(1), trap);
+      b(Condition(0xE), &done, Label::kNear);  // normal case
+      lghi(dst.gp(), Operand::Zero());
+      bind(&done);
+      return true;
+    }
+    case kExprF32SConvertI64:
+      ConvertInt64ToFloat(dst.fp(), src.gp());
+      return true;
+    case kExprF32UConvertI64:
+      ConvertUnsignedInt64ToFloat(dst.fp(), src.gp());
+      return true;
+    case kExprI64SConvertF64: {
+      ConvertDoubleToInt64(dst.gp(), src.fp()); // f64 -> i64 round to zero.
+      b(Condition(1), trap);
+      return true;
+   }
+    case kExprI64UConvertF64: {
+      ConvertDoubleToUnsignedInt64(dst.gp(), src.fp());
+      b(Condition(1), trap);
+      return true;
+    }
+    default:
+      UNREACHABLE();
+  }
   return true;
 }
 
 void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
-  bailout(kUnsupportedArchitecture, "emit_i32_signextend_i8");
+  lbr(dst, src);
 }
 
 void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {
-  bailout(kUnsupportedArchitecture, "emit_i32_signextend_i16");
+  lhr(dst, src);
 }
 
 void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,
                                               LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i64_signextend_i8");
+  LoadB(dst.gp(), src.gp());
 }
 
 void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,
                                                LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i64_signextend_i16");
+  LoadHalfWordP(dst.gp(), src.gp());
 }
 
 void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,
                                                LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i64_signextend_i32");
+  LoadW(dst.gp(), src.gp())
 }
 
 void LiftoffAssembler::emit_jump(Label* label) {
-  bailout(kUnsupportedArchitecture, "emit_jump");
+  b(al, label);
 }
 
 void LiftoffAssembler::emit_jump(Register target) {
-  bailout(kUnsupportedArchitecture, "emit_jump");
+  Jump(target)
 }
 
 void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,
                                       ValueType type, Register lhs,
                                       Register rhs) {
-  bailout(kUnsupportedArchitecture, "emit_cond_jump");
+  Condition liftoff_cond = liftoff::LiftoffCondToCond(cond);
+  switch (type) {
+    case kWasmI32: {
+      if (rhs == no_reg) {
+        printf("emit cond jump  I32 noreg\n");
+        if (liftoff_cond == cond) {
+           CmpLogical32(lhs, Operand(0));
+        } else {
+           Cmp32(lhs, Operand(0));
+        }
+      } else {
+        printf("emit cond jump  I32\n");
+        if (liftoff_cond == cond) {
+           CmpLogical32(lhs, rhs);
+        } else {
+           Cmp32(lhs, rhs);
+        }
+      }
+      break; }
+    case kWasmI64: {
+      if (rhs == no_reg) {
+        printf("emit cond jump  I64 noreg\n");
+        if (liftoff_cond == cond) {
+           CmpLogicalP(lhs, Operand(0));
+        } else {
+           CmpP(lhs, Operand(0));
+        }
+
+      } else {
+        printf("emit cond jump  I64 \n");
+        if (liftoff_cond == cond) {
+           CmpLogicalP(lhs, rhs);
+        } else {
+           CmpP(lhs, rhs);
+        }
+      }
+      break; }
+    default:
+      UNREACHABLE();
+  }
+  b(liftoff_cond, label);
 }
 
 void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {
-  bailout(kUnsupportedArchitecture, "emit_i32_eqz");
+  emit_i32_clz(dst, src);
+  ShiftRight(dst, dst, Operand(5));
 }
 
 void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,
                                          Register lhs, Register rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i32_set_cond");
+  Condition liftoff_cond = liftoff::LiftoffCondToCond(cond);
+  if (liftoff_cond != cond){
+     //Sign Comparaion.
+     Cmp32(lhs, rhs);
+  } else {
+     CmpLogical32(lhs, rhs);
+  }
+  mov(dst, Operand(0));
+  mov(r0, Operand(1));
+  locr(liftoff_cond, dst, r0);
 }
 
 void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i64_eqz");
+  Label done;
+  mov(dst, Operand(0));
+  CmpP(src.gp(), Operand(0));
+  bne(&done);
+  mov(dst, Operand(1));
+  bind(&done);
 }
 
 void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,
                                          LiftoffRegister lhs,
                                          LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i64_set_cond");
+  Condition liftoff_cond = liftoff::LiftoffCondToCond(cond);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  if (liftoff_cond == cond) {
+     CmpLogicalP(lhs.gp(), rhs.gp());
+  } else {
+     CmpP(lhs.gp(), rhs.gp());
+  }
+  mov(dst, Operand(0));
+  mov(scratch, Operand(1));
+  LoadOnConditionP(liftoff_cond, dst, scratch);
 }
 
 void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,
                                          DoubleRegister lhs,
                                          DoubleRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_f32_set_cond");
+  cond = liftoff::LiftoffCondToCond(cond);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  mov(dst, Operand(0));
+  mov(scratch, Operand(1));
+  cebr(lhs, rhs);
+  LoadOnConditionP(cond, dst, scratch);
+  if (cond != ne){
+      mov(scratch, Operand(0));
+      LoadOnConditionP(overflow, dst, scratch);
+  }
 }
 
 void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,
                                          DoubleRegister lhs,
                                          DoubleRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_f64_set_cond");
+  cond = liftoff::LiftoffCondToCond(cond);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  mov(dst, Operand(0));
+  mov(scratch, Operand(1));
+  cdbr(lhs, rhs);
+  LoadOnConditionP(cond, dst, scratch);
+  if (cond != ne){
+      mov(scratch, Operand(0));
+      LoadOnConditionP(overflow, dst, scratch);
+  }
 }
 
 void LiftoffAssembler::StackCheck(Label* ool_code, Register limit_address) {
-  bailout(kUnsupportedArchitecture, "StackCheck");
+  LoadP(limit_address, MemOperand(limit_address));
+  CmpLogicalP(sp, limit_address);
+  b(le, ool_code);
 }
 
 void LiftoffAssembler::CallTrapCallbackForTesting() {
-  bailout(kUnsupportedArchitecture, "CallTrapCallbackForTesting");
+  PrepareCallCFunction(0, 0,  no_reg);
+  CallCFunction(ExternalReference::wasm_call_trap_callback_for_testing(), 0);
 }
 
 void LiftoffAssembler::AssertUnreachable(AbortReason reason) {
-  bailout(kUnsupportedArchitecture, "AssertUnreachable");
+  TurboAssembler::AssertUnreachable(reason);
 }
 
 void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {
-  bailout(kUnsupportedArchitecture, "PushRegisters");
+  MultiPush(regs.GetGpList());
+  MultiPushDoubles(regs.GetFpList());
 }
 
 void LiftoffAssembler::PopRegisters(LiftoffRegList regs) {
-  bailout(kUnsupportedArchitecture, "PopRegisters");
+  MultiPop(regs.GetGpList());
+  MultiPopDoubles(regs.GetFpList());
 }
 
 void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) {
-  bailout(kUnsupportedArchitecture, "DropStackSlotsAndRet");
+  Drop(num_stack_slots);
+  Ret();
 }
 
 void LiftoffAssembler::CallC(wasm::FunctionSig* sig,
@@ -520,35 +1298,181 @@ void LiftoffAssembler::CallC(wasm::FunctionSig* sig,
                              const LiftoffRegister* rets,
                              ValueType out_argument_type, int stack_bytes,
                              ExternalReference ext_ref) {
-  bailout(kUnsupportedArchitecture, "CallC");
+  // Reserve space in the stack.
+  lay(sp, MemOperand(sp, -stack_bytes));
+
+  int arg_bytes = 0;
+  for (ValueType param_type : sig->parameters()) {
+    switch (param_type) {
+      case kWasmI32:
+        printf("CallC kWasmI32\n");
+        StoreW(args->gp(), MemOperand(sp, arg_bytes));
+        break;
+      case kWasmI64:
+        printf("CallC kWasmI64\n");
+        StoreP(args->gp(), MemOperand(sp, arg_bytes));
+        break;
+      case kWasmF32:
+        printf("CallC kWasmF32\n");
+        StoreFloat32(args->fp(), MemOperand(sp, arg_bytes));
+        break;
+      case kWasmF64:
+        printf("CallC kWasmF64\n");
+        StoreDouble(args->fp(), MemOperand(sp, arg_bytes));
+        break;
+      default:
+        UNREACHABLE();
+    }
+    args++;
+    arg_bytes += ValueTypes::MemSize(param_type);
+  }
+  DCHECK_LE(arg_bytes, stack_bytes);
+
+  // Pass a pointer to the buffer with the arguments to the C function.
+  LoadRR(r2, sp);
+
+  // Now call the C function.
+  constexpr int kNumCCallArgs = 1;
+  PrepareCallCFunction(kNumCCallArgs, no_reg);
+  CallCFunction(ext_ref, kNumCCallArgs);
+
+  // Move return value to the right register.
+  const LiftoffRegister* result_reg = rets;
+  if (sig->return_count() > 0) {
+    DCHECK_EQ(1, sig->return_count());
+    constexpr Register kReturnReg = r2;
+    if (kReturnReg != rets->gp()) {
+      Move(*rets, LiftoffRegister(kReturnReg), sig->GetReturn(0));
+    }
+    result_reg++;
+  }
+
+  // Load potential output value from the buffer on the stack.
+  if (out_argument_type != kWasmStmt) {
+    switch (out_argument_type) {
+      case kWasmI32:
+        LoadW(result_reg->gp(), MemOperand(sp));
+        break;
+      case kWasmI64:
+        LoadP(result_reg->gp(), MemOperand(sp));
+        break;
+      case kWasmF32:
+        LoadFloat32(result_reg->fp(), MemOperand(sp));
+        break;
+      case kWasmF64:
+        LoadDouble(result_reg->fp(), MemOperand(sp));
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+  lay(sp, MemOperand(sp, stack_bytes));
 }
 
 void LiftoffAssembler::CallNativeWasmCode(Address addr) {
-  bailout(kUnsupportedArchitecture, "CallNativeWasmCode");
+  Call(addr, RelocInfo::WASM_CALL);
 }
 
 void LiftoffAssembler::CallIndirect(wasm::FunctionSig* sig,
                                     compiler::CallDescriptor* call_descriptor,
                                     Register target) {
-  bailout(kUnsupportedArchitecture, "CallIndirect");
+  DCHECK(target != no_reg);
+  Call(target);
 }
 
 void LiftoffAssembler::CallRuntimeStub(WasmCode::RuntimeStubId sid) {
-  bailout(kUnsupportedArchitecture, "CallRuntimeStub");
+  // A direct call to a wasm runtime stub defined in this module.
+  // Just encode the stub index. This will be patched at relocation.
+  Call(static_cast<Address>(sid), RelocInfo::WASM_STUB_CALL);
 }
 
 void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) {
-  bailout(kUnsupportedArchitecture, "AllocateStackSlot");
+  size = RoundUp(size, 16);
+  lay(sp, MemOperand(sp, -size));
+  TurboAssembler::Move(addr, sp);
 }
 
 void LiftoffAssembler::DeallocateStackSlot(uint32_t size) {
-  bailout(kUnsupportedArchitecture, "DeallocateStackSlot");
+  size = RoundUp(size, 16);
+  lay(sp, MemOperand(sp, size));
 }
 
 void LiftoffAssembler::DebugBreak() { stop(); }
 
 void LiftoffStackSlots::Construct() {
-  asm_->bailout(kUnsupportedArchitecture, "LiftoffStackSlots::Construct");
+  for (auto& slot : slots_) {
+    const LiftoffAssembler::VarState& src = slot.src_;
+    switch (src.loc()) {
+      case LiftoffAssembler::VarState::kStack: {
+        switch (src.type()) {
+          case kWasmI32: {
+            UseScratchRegisterScope temps(asm_);
+            Register scratch = temps.Acquire();
+            asm_->LoadW(scratch, liftoff::GetStackSlot(slot.src_offset_)); 
+            asm_->Push(scratch);
+            break;
+          }
+          case kWasmI64: {
+            UseScratchRegisterScope temps(asm_);
+            Register scratch = temps.Acquire();
+            asm_->LoadP(scratch, liftoff::GetStackSlot(slot.src_offset_));
+            asm_->Push(scratch);
+            break;
+          }
+          case kWasmF32: {
+            asm_->LoadFloat32(kScratchDoubleReg,
+                      liftoff::GetStackSlot(slot.src_offset_));
+            asm_->push(kScratchDoubleReg);
+            break;
+          }
+          case kWasmF64: {
+            asm_->LoadDouble(kScratchDoubleReg, liftoff::GetStackSlot(slot.src_offset_));
+            asm_->push(kScratchDoubleReg);
+            break;
+          }
+          default:
+            UNREACHABLE();
+        }
+        break;
+      }
+      case LiftoffAssembler::VarState::kRegister:
+        switch (src.type()) {
+          case kWasmI64:
+          case kWasmI32:
+            asm_->push(src.reg().gp());
+            break;
+          case kWasmF32:
+            asm_->push(src.reg().fp());
+            break;
+          case kWasmF64:
+            asm_->push(src.reg().fp());
+            break;
+          default:
+            UNREACHABLE();
+        }
+        break;
+      case LiftoffAssembler::VarState::kIntConst: {
+        DCHECK(src.type() == kWasmI32 || src.type() == kWasmI64);
+        UseScratchRegisterScope temps(asm_);
+        Register scratch = temps.Acquire();
+
+        switch (src.type()) {
+            case kWasmI32 : {
+                asm_->mov(scratch, Operand(src.i32_const()));
+                break;
+            }
+            case kWasmI64: {
+                asm_->mov(scratch, Operand(int64_t{slot.src_.i32_const()}));
+                break;
+            }
+            default:
+              UNREACHABLE();
+            }
+        asm_->push(scratch);
+        break;
+      }
+    }
+  }
 }
 
 }  // namespace wasm
diff --git a/src/wasm/wasm-value.h b/src/wasm/wasm-value.h
index 9a6f0ca726..250eba1c82 100644
--- a/src/wasm/wasm-value.h
+++ b/src/wasm/wasm-value.h
@@ -76,7 +76,7 @@ class WasmValue {
   explicit WasmValue(ctype v) : type_(localtype), bit_pattern_{} {            \
     static_assert(sizeof(ctype) <= sizeof(bit_pattern_),                      \
                   "size too big for WasmValue");                              \
-    base::WriteUnalignedValue<ctype>(reinterpret_cast<Address>(bit_pattern_), \
+    base::WriteLittleEndianValue<ctype>(reinterpret_cast<Address>(bit_pattern_), \
                                      v);                                      \
   }                                                                           \
   ctype to_##name() const {                                                   \
@@ -84,7 +84,7 @@ class WasmValue {
     return to_##name##_unchecked();                                           \
   }                                                                           \
   ctype to_##name##_unchecked() const {                                       \
-    return base::ReadUnalignedValue<ctype>(                                   \
+    return base::ReadLittleEndianValue<ctype>(                                   \
         reinterpret_cast<Address>(bit_pattern_));                             \
   }
   FOREACH_WASMVAL_TYPE(DEFINE_TYPE_SPECIFIC_METHODS)
-- 
2.22.0

